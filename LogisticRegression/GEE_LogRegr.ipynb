{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><font size=\"6\"><b> Logistic Regression Algorithm </b></font></center>\n",
    " \n",
    "<br>\n",
    "<center><font size=\"5\"><b> Jade Gee  </b></font></center>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<font size =\"3\"> Logistic Regression Model is an supervised machine learning algorithm that uses a logistic function to model a binary dependent variable. The probability of an event---i.e. pass/fail, win/lose, etc.---is what is modeled. \n",
    "\n",
    "Logistic Regression is a form of binary regression, where the model has a dependent variable that is categorical (it has only two (2) possible values), `0` or `1`; and the logarithm of the odds (log-odds) for the value labeled `1` is a linear combination of at least one independent variables or predictors. From this, our logistic function takes the log-odds and converts it into probability. The factor that defines the logistic model is that one of the independent variables multiplicatively scales the odds of the given outcome at a *constant* rate. Each independent variable has their own parameter which, for a binary dependent variable, generalizes the odds ratio.\n",
    "\n",
    "**NOTE:** Logistic regression algorithm is not a classifier as it does not perform classification of the data, but it can be used to make a classifier.\n",
    "\n",
    "Although this algorithm has various types, such as\n",
    "\n",
    "+ **Binary Logistic Regression**\n",
    "\n",
    "+ **Multinomial Logistic Regression**, and\n",
    "\n",
    "+ **Ordinal Logistic Regression**\n",
    "\n",
    "for the purpose of this notebook, we are going to focus on the first type of logistic regression: binary.\n",
    "\n",
    "To begin we will take a look at the data set, `Admission_Predict_Ver1.1`, which can be located [here](https://www.kaggle.com/yameenajani/admission). From this data set, we will take a look at the following information </font>\n",
    "\n",
    "<font size=\"4\">$$\n",
    "\\begin{align}\n",
    "(x^1, y^1) \\text{, ... , } (x^n, y^n); where& \\\\\n",
    "&x^{(i)} = \n",
    "\\begin{bmatrix}\n",
    "\\text{student; GRE score} \\\\\n",
    "\\text{student; TOEFL score} \\\\\n",
    "\\text{student; CGPA}\n",
    "\\end{bmatrix}\\\\\n",
    "&y^{(i)} \\in \\{0, 1 \\} \\text{  where } 1 \\text{ indicates Chance of Admit; } 0 \\text{ if not}\n",
    "\\end{align}\n",
    "$$</font>\n",
    "\n",
    "### Logistic Regression Formula\n",
    "\n",
    "![Logistic Regression](LogR.png)\n",
    "\n",
    "<font size =\"3\"> This translates to the following formula\n",
    "\n",
    "<font size=\"5\">$$\n",
    "\\begin{align}\n",
    "\\hat y^{(i)} = \\sigma (w^T x^{(i)} + b)\n",
    "\\end{align}\n",
    "$$</font>\n",
    "\n",
    "With this formula at hand, we will proceed to gather our data in preparation for the implementation of our binary logistic regression algorithm. </font>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Gather the Data\n",
    "\n",
    "<font size =\"3\"> In order to begin, we will need to import the following libraries:\n",
    "    \n",
    "+ `Random`\n",
    "    - To create a random subset of our data\n",
    "    \n",
    "+ `CSV` and `DataFrames`\n",
    "    - To import our data set as a data frame </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.read(\"Admission_Predict_Ver1.1.csv\", DataFrame);\n",
    "\n",
    "x_data = [[x[1], x[2], x[3]] for x in zip(data.GRE Score, data.TOEFL Score, data.CGPA)];\n",
    "y_data = [x for x in data.Chance of Admit];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"3\"> Now, we need to split the data into a training subset and a testing subset. In order to do this we will use `randsubseq` from the `Random` library to randomly select data points from the data set that have a probability of 0.5. These selected data points will be designated as the training data; and, all of the points in the original data, `data`, that are not in `training_data` will become the testing data, `test_data`. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into a Training set and a Testing Set\n",
    "# Randomly select data points from the full data set\n",
    "# to make a training data set\n",
    "\n",
    "full_data = [x for x in zip(x_data, y_data)];\n",
    "\n",
    "# Randomly selects the data points from the original data\n",
    "training_data = randsubseq(full_data, 0.5);\n",
    "\n",
    "# Takes all points in the original data that are not in \n",
    "# the training data and stores them as the test data\n",
    "test_data = [x for x in full_data if x ∉ training_data];\n",
    "\n",
    "# Assigns the first column to the x-values\n",
    "training_x = [x[1] for x in training_data];\n",
    "test_x = [x[1] for x in full_data if x ∉ training_data];\n",
    "\n",
    "# Assigns the second column to the y-values\n",
    "training_y = [x[2] for x in training_data];\n",
    "test_y = [x[2] for x in full_data if x ∉ training_data];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Implement the Algorithm\n",
    "\n",
    "<font size =\"3\"> In order to implement the algorithm, we will need to create several functions to compute the following:</font>\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "<font size =\"3\">$$\n",
    "\\begin{align}\n",
    "L_{CE}(\\hat y^{(i)}, y^{(i)}) = \\hat y^{(i)} + (1-y^{(i)}) log(1-y^{(i)})\n",
    "\\end{align}\n",
    "$$</font>\n",
    "\n",
    "---\n",
    "\n",
    "### Average Loss\n",
    "\n",
    "<font size =\"3\">$$\n",
    "\\begin{align}\n",
    "Cost(w,b) = \\frac{1}{N} \\sum_{i=1}^N \\hat y^{(i)} + (1-y^{(i)}) log(1-y^{(i)})\n",
    "\\end{align}\n",
    "$$</font>\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient of Cost Function --- Weights\n",
    "\n",
    "<font size =\"3\">$$\n",
    "\\frac {\\partial L_{CE}(w,b)}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\sigma (w^T x^{(i)} + b) - y\\right]x^{(i)}_{j}\n",
    "$$</font>\n",
    "\n",
    "---\n",
    "\n",
    "### Bias\n",
    "\n",
    "<font size =\"3\">$$\n",
    "\\frac {\\partial L_{CE}(w,b)}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\sigma (w^T x^{(i)} + b) - y\\right]\n",
    "$$</font>\n",
    "\n",
    "<font size =\"3\"> As such, the following functions have been created to calculate the above formulas for the cross-entropy loss, average loss, gradient of the cost function or weights, and bias. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error_MSE"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "     x:    x values in the data\n",
    "\n",
    "This function takes in x-values and calculates\n",
    "the value of σ.\n",
    "\n",
    "Returns: \n",
    "    The value of σ.\n",
    "\"\"\"\n",
    "σ(x) = 1/(1 + exp(-x))\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "     x:    x values in the data\n",
    "     y:    y values in the data\n",
    "     w:    weight\n",
    "     b:    bias\n",
    "\n",
    "This function takes in x-values, y-values, weight,\n",
    "and bias and calculates the cross entropy loss.\n",
    "\n",
    "Returns: \n",
    "    The cross entropy loss.\n",
    "\"\"\"\n",
    "function cross_entropy_loss(x, y, w, b)\n",
    "    return -y * log(σ(w'x + b)) - (1-y) * log(1 - σ(w'x + b))\n",
    "end\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "     x:    x values in the data\n",
    "     y:    y values in the data\n",
    "     w:    weight\n",
    "     b:    bias\n",
    "\n",
    "This function takes in x-values, y-values, weight,\n",
    "and bias and calculates the average loss for the data.\n",
    "\n",
    "Returns: \n",
    "    The average loss for the data set.\n",
    "\"\"\"\n",
    "function avg_loss(x, y, w, b)\n",
    "    N = length(x)\n",
    "    return (1/N) * sum([cross_entropy_loss(x[i], y[i], w, b) for i = 1:N])\n",
    "end\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "     features:  x values in the data\n",
    "     labels:    y values in the data\n",
    "     w:         weight\n",
    "     b:         bias\n",
    "     α:         step length\n",
    "\n",
    "Calculates the new weight and bias and updates them.\n",
    "\n",
    "Returns:\n",
    "    The new weight and bias generated after the derivative change.\n",
    "\"\"\"\n",
    "function logit_batch_gradient_descent(features, labels, w, b, α)\n",
    "    \n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    \n",
    "    N = length(features)\n",
    "    \n",
    "    for i = 1:N\n",
    "        del_w += (σ(w' * features[i] + b) - labels[i]) * features[i]\n",
    "        del_b += (σ(w' * features[i] + b) - labels[i])\n",
    "    end\n",
    "    \n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    \n",
    "    return w, b\n",
    "end\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "     x_data:    x values in the data\n",
    "     y_data:    y values in the data\n",
    "     w:         weight\n",
    "     b:         bias\n",
    "     α:         step length\n",
    "     iter:      number of iterations to complete\n",
    "\n",
    "Trains the regression and displays the cost at 10^n iterations.\n",
    "\n",
    "Returns:\n",
    "    The new weight and bias generated.\n",
    "\"\"\"\n",
    "function logit_training(features, labels, w, b, α, iter)\n",
    "    j = 0\n",
    "    for i = 1:iter\n",
    "        w, b = logit_batch_gradient_descent(features, labels, w, b, α)      \n",
    "        if i == 10^j\n",
    "            println(i, \" iteration with cost \", avg_loss(x_data, y_data, w, b))\n",
    "            j = j + 1\n",
    "        end\n",
    "    end\n",
    "        return w, b\n",
    "end\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "     x:    x values in the data\n",
    "     y:    y values in the data\n",
    "     w:    weight\n",
    "     b:    bias\n",
    "\n",
    "This function takes in x-values, y-values, weight,\n",
    "and bias and predicts the label of an x-value.\n",
    "\n",
    "Returns: \n",
    "    The prediction of the label associated with the x values.\n",
    "\"\"\"\n",
    "function logit_predictor(x, y, w, b)\n",
    "    if σ(w'x + b) >= 0.5\n",
    "        println(\"Prediction:\\tAccepted\")\n",
    "        y == 1 ? println(\"Actual:\\t\\tAccepted\") : println(\"Actual:\\t\\tNot Accepted\")\n",
    "        return 1\n",
    "    else\n",
    "        println(\"Prediction:\\tAccepted\")\n",
    "        y == 1 ? println(\"Actual:\\t\\tAccepted\") : println(\"Actual:\\t\\tNot Accepted\")\n",
    "        return 0\n",
    "    end\n",
    "end\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "     x:    x values in the data\n",
    "     y:    y values in the data\n",
    "     w:    weight\n",
    "     b:    bias\n",
    "\n",
    "This function takes in x-values, y-values, weight,\n",
    "and bias and calculates the mean square error.\n",
    "\n",
    "Returns: \n",
    "    The mean squared error.\n",
    "\"\"\"\n",
    "function error_MSE(x, y, w, b)\n",
    "    mean_error = 0.0\n",
    "\n",
    "    for i = 1:length(x)\n",
    "        mean_error = mean_error + (logit_predictor(x[i], y[i], w, b) - y[i])^2\n",
    "    end\n",
    "    println(\"--------------------------------\")\n",
    "    println(\"\\tError: \\t\", mean_error/length(x_data))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train the algorithm\n",
    "\n",
    "<font size =\"3\"> Now, we will train the algorithm using initial weights `w = [0.0, 0.0, 0.0]` and `b = 0.0` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.6975379877487258\n",
      "10 iteration with cost 1.1952527047242898\n",
      "100 iteration with cost 1.194655298975357\n",
      "1000 iteration with cost 1.1884432383569965\n",
      "10000 iteration with cost 1.1270430520806354\n",
      "100000 iteration with cost 0.5470366829725104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.007510038757002728, 0.29456618666821893, 0.8549193300115617], -0.09129887445783746)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Pass with the Training Data\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "\n",
    "w, b = logit_training(training_x, training_y, w, b, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"3\"> After the first pass of the algorithm, we can see that our algorithm does take steps toward in the direction of the descent; but, it does not reach a minimum. For the purpose of this notebook, we undertake $n$ number of passes to show the descent of the average loss. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.5641669430777728\n",
      "10 iteration with cost 0.5469874687362811\n",
      "100 iteration with cost 0.5465448555347018\n",
      "1000 iteration with cost 0.5421499489744581\n",
      "10000 iteration with cost 0.5013556324124026\n",
      "100000 iteration with cost 0.4193408916975182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.007292399353612156, 0.4582055956281985, 1.0187689688481552], -0.16975992913114293)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass 2 -- Training Data\n",
    "w, b = logit_training(training_x, training_y, w, b, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.41934078427122057\n",
      "10 iteration with cost 0.4193398174604494\n",
      "100 iteration with cost 0.41933015191644385\n",
      "1000 iteration with cost 0.41923375178762656\n",
      "10000 iteration with cost 0.41829426253607105\n",
      "100000 iteration with cost 0.4106263236447304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.00822135217241391, 0.5963459104948603, 1.0894095584713905], -0.24555751740614856)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass 3 -- Training Data\n",
    "w, b = logit_training(training_x, training_y, w, b, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.4106262507833631\n",
      "10 iteration with cost 0.41062559504015045\n",
      "100 iteration with cost 0.4106190385078476\n",
      "1000 iteration with cost 0.4105535629119764\n",
      "10000 iteration with cost 0.40990753236120003\n",
      "100000 iteration with cost 0.4041339092170203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.008905484266453564, 0.71913120506564, 1.127757688322977], -0.31972757711370803)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass 4 -- Training Data\n",
    "w, b = logit_training(training_x, training_y, w, b, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test the algorithm\n",
    "\n",
    "<font size =\"3\"> Now that we have trained our algorithm and seen the data, we can test our algorithm on the test data. We will perform the same number of passes as we did for our training data to see if our average loss is the same or near the training data using initial weights `w = [0.0, 0.0, 0.0]` and `b = 0.0`. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.7355410326666838\n",
      "10 iteration with cost 0.7248417625064386\n",
      "100 iteration with cost 0.7245157538820408\n",
      "1000 iteration with cost 0.7212818993649188\n",
      "10000 iteration with cost 0.6914229078302015\n",
      "100000 iteration with cost 0.5400892021140317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0029937810348607406, 0.08617063453120022, 0.39897801829697793], -0.016070251197478236)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Pass of the Test Data\n",
    "w2 = [0.0, 0.0, 0.0]\n",
    "b2 = 0.0\n",
    "\n",
    "w2, b2 = logit_training(test_x, test_y, w2, b2, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.5400883516523918\n",
      "10 iteration with cost 0.5400806979637801\n",
      "100 iteration with cost 0.5400042072016865\n",
      "1000 iteration with cost 0.5392438876520876\n",
      "10000 iteration with cost 0.5320762655495221\n",
      "100000 iteration with cost 0.4883690993229899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.004197740938538153, 0.1478548901497723, 0.5884590789303132], -0.029200113080882134)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass 2 -- Test Data \n",
    "w2, b2 = logit_training(test_x, test_y, w2, b2, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.48836879280677564\n",
      "10 iteration with cost 0.48836603427823094\n",
      "100 iteration with cost 0.4883384606091755\n",
      "1000 iteration with cost 0.48806388096947223\n",
      "10000 iteration with cost 0.4854293635586577\n",
      "100000 iteration with cost 0.4669884614021719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.004942072125095563, 0.19996052982472592, 0.6916194042963345], -0.041069677943505176)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass 3 -- Test Data\n",
    "w2, b2 = logit_training(test_x, test_y, w2, b2, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration with cost 0.4669883129534147\n",
      "10 iteration with cost 0.46698697695566244\n",
      "100 iteration with cost 0.46697362104237494\n",
      "1000 iteration with cost 0.46684046707831356\n",
      "10000 iteration with cost 0.46554822497362497\n",
      "100000 iteration with cost 0.4556119163568081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0054547281261891085, 0.24738823691224093, 0.7522536951745404], -0.052311653433813894)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass 4 -- Test Data\n",
    "w2, b2 = logit_training(test_x, test_y, w2, b2, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test the Predictions\n",
    "\n",
    "<font size =\"3\"> With out operational algorithm, we can now implement our predictor function on the training data. We must iterate through the data set to test the prediction and compare it to the actual data in our `training_y` data or the column 2 data from the `training_data`. Then we will do the same for the test data comparing it to the actual values in `test_y`. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n"
     ]
    }
   ],
   "source": [
    "# Predict acceptance test with the training data\n",
    "\n",
    "for i = 1: length(training_x)\n",
    "    logit_predictor(training_x[i], training_y[i], w, b)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n"
     ]
    }
   ],
   "source": [
    "# Predict acceptance test with the test data\n",
    "\n",
    "for i = 1: length(test_x)\n",
    "    logit_predictor(test_x[i], test_y[i], w, b)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"3\"> As we can see from the prediction comparison for both the `training_data` and the `test_data`, the percentage of incorrect predictions appear to be about the same. To verify this, we will calculate the error for both subset of the data for comparison. </font>\n",
    "\n",
    "---\n",
    "\n",
    "## Calculate the Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "--------------------------------\n",
      "\tError: \t0.1\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Error of the training data\n",
    "error_MSE(training_x, training_y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tAccepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "Prediction:\tAccepted\n",
      "Actual:\t\tNot Accepted\n",
      "--------------------------------\n",
      "\tError: \t0.075\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Error of the test data\n",
    "error_MSE(test_x, test_y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "<font size =\"3\"> As we can see from above, the errors calculated for both of the subsets of the data are extremely close. This allows us to see that we have successfully implemented the Logistic Regression Algorithm on our data. </font>\n",
    "\n",
    "\n",
    "### For more information on Logistic Regression, please see:  \n",
    "<br>\n",
    "\n",
    "<font size =\"3\"> \n",
    "    \n",
    "+ [Logistic Regression - Towards Data Science](https://towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559)\n",
    "<br>\n",
    "    \n",
    "+ [Logistic Regression - Statistic Solution](https://www.statisticssolutions.com/what-is-logistic-regression/)\n",
    "<br>\n",
    "    \n",
    "+ [Logistic Regression - Machine Learning Mastery](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n",
    "<br>\n",
    "    \n",
    "+ [Logistic Regression - Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\n",
    "</font>\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
