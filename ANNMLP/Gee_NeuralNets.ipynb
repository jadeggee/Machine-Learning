{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifical Neural Network using Multilayer Perceptron\n",
    "## Jade Gee\n",
    "---\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "<font size=\"3\">Deep Neural Networks were inspired by biological neurons. Biological neurons receive signals through the dendrites and send them through to the nucleus for stimulation which then, depending on the signal type, activate a signal that goes through the axiom.\n",
    "\n",
    "![Neurons](neurons.png)\n",
    "\n",
    "Like a biological neuron, the artificial neuron takes weighted inputs that go into the dendrites and stimulate the nucleus which then cause the axiom to fire. These artifical neurons form our neural networks and produce extremely complex functions.\n",
    "\n",
    "---\n",
    "## Gather the Data\n",
    "\n",
    "For the purpose of this Neural Network, we are going to be using the MNIST data from the `MLDatasets` package. This dataset comes from the **MNIST Database** from the National Institute of Standards and Technology, and is considered to be one of the largest databases of handwritten digits that are typically used for training and testing data in machine learning. An example of the data in the MNIST database can be found below, and for additional information regarding the MNIST data, please look [here](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "    \n",
    "  ![MNISTexample](mnistExample.png)\n",
    "\n",
    "We will also need the following packages to access and display the data: </font>\n",
    "\n",
    "<font size=\"4\">+ `MLDatasets` </font>\n",
    "<br><font size=\"3\">\n",
    "> [Documentation](https://juliaml.github.io/MLDatasets.jl/latest/)</font>\n",
    "    \n",
    "<font size=\"4\">+ `Images`, `TestImages`, `ImageMagick`</font>\n",
    "<br> <font size=\"3\">\n",
    "> [Images Documentation](https://juliaimages.org/stable/)\n",
    "<br>\n",
    "> [TestImages Docuentation](https://testimages.juliaimages.org/)\n",
    "<br>\n",
    "> [ImageMagick Documentation](https://imagemagick.org/)\n",
    "    \n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using Images\n",
    "using TestImages\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Gather the data\n",
    "\n",
    "<font size=\"3\">The first step we must take is to designate our training and testing data as shown in the following code cell.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first image in our train data is labeled 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAIKSURBVGje7dpPiI1RGMfxzyAL8qfZmFISi1EiFigpSZJiMbGhbLBDVjZ2FqSwQBazUhayxYryd6FuTf5syN6fHYM0yGDxvJPr3tu9c2eKc0/nW29v5z3ve3/93qdz3uc551IoFAqFQqHQ+/R1+8BMLKhrH8EcDOIwzmEvvuIMTjY8P+NfO8xfcFanG5ZgNjZiExZid4v7XuMihvAZz/EwBYf5C7Ydh2tx19/jrhU/cQBfqvZbfMCrFBzmL9g2hv2oYVmLvhpGsQXfdY7zf3OYv2DbufQ9jmMnnoq5Ep5hmxh3K3EsZYf5C04qp5kvvnHDOIj9uNYrDvMX7JjTwKfq/LE6H8J18R1M3mH+gl3VFnNxC5uxA3d6wWH+gl3Xh8vxROQz9zGCy/iVqsP8BbuOIVEDXsG8qn0CV/EuRYf5C04phrAK57G1ag/jFN6k5jB/wSnHkFiz2SXGZB/uiZojKYf5C04rhhN8EwnuD2zHg5Qc5i84qdqiFauxB+vqfuQFHqXmMH/BrmM4iKMirxmouz4ucppONWP+rzTduXQA+8T+0tKGvhGRz9xM0WH+gh1juEisiV7Cioa+Gs7ihsmv2eT/StOJYb+oF9Zo3rd4LOqK2xhL3WH+gk0x3CD2KtZjcUPfGC7gtD97hck7zF+wKacZqo4JXoo10nHxP4vRXnOYv2ChUCgUps9vDE1MYMzifHwAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 reinterpret(Gray{N0f8}, ::LinearAlgebra.Adjoint{Normed{UInt8,8},Array{Normed{UInt8,8},2}}):\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = MNIST.traindata();\n",
    "test_x, test_y = MNIST.testdata();\n",
    "\n",
    "println(\"The first image in our train data is labeled \", train_y[1])\n",
    "colorview(Gray, train_x[:, :, 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structures `train_x` and `train_y` are stored as 3-dimensional tensors of dimensions: $28 \\times 28 \\times 60000$. The images in the data set are stored as $28 \\times 28$ grey scaled grid  of pixel values.\n",
    "\n",
    "In order to pass these images into our neural network, we must flatten the training matrices into vectors as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     3,
     17
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array{Tuple{Array{Normed{UInt8,8},1},Array{Float64,1}},1}\n",
      "\n",
      "Array{Tuple{Array{Normed{UInt8,8},1},Array{Float64,1}},1}"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i = 1:60000\n",
    "    push!(X, reshape(train_x[:, :, i], 784))\n",
    "    y = zeros(10)\n",
    "    y[train_y[i] + 1] = 1.0\n",
    "    push!(Y, y)\n",
    "end\n",
    "\n",
    "train_data = [x for x in zip(X, Y)];\n",
    "print(typeof(train_data))\n",
    "println(\"\\n\")\n",
    "\n",
    "X2 = []\n",
    "Y2 = []\n",
    "\n",
    "for i = 1:10000\n",
    "    push!(X2, reshape(test_x[:, :, i], 784))\n",
    "    y2 = zeros(10)\n",
    "    y2[test_y[i] + 1] = 1.0\n",
    "    push!(Y2, y2)\n",
    "end\n",
    "\n",
    "test_data = [x for x in zip(X2, Y2)];\n",
    "print(typeof(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Network Architecture\n",
    "\n",
    "<font size=\"3\">We are using a multilayered perceptron model that contains $L$ layers, $784$ input neurons, $L-2$ hidden layers of an arbitrary size, and $10$ neurons in the output layer. Our activation function will utilize the sigmoid function.\n",
    "\n",
    "+ Sigmoid Function:\n",
    "\n",
    "    $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "![MultilayeredPerceptron](multi_p.png)\n",
    "\n",
    "For $l=2$,... , $L$, each layer $l$ in our network will have two phases, the pre-activation phase\n",
    "\n",
    "$$ z^l = W^l a^{l-1} + b^l, $$ \n",
    "and the post-activation phase \n",
    "$$ a^l = \\sigma(z^l). $$\n",
    "\n",
    "The pre-activation phase consists of a weighted linear combination of post-activation values int he previous layer while the post-activation values consists of passing the pre-activation value through an activation function element-wise. ($ a^1 = x^{(i)}$, where $x^{(i)}$ is the current input feature vector of our network.\n",
    "\n",
    "For a given instance, $(x^{(i)}, y^{(i)})$, we will use the Mean Square Error cost: \n",
    "$$ C = C(W, b) = \\frac{1}{2} \\sum_{k=1}^n (a_k^L - y_k^{(i)})^2,$$\n",
    "where $n=10$ is the size of the output layer. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_network (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation function and its derivative definitions\n",
    "σ(x) = 1.0/(1.0 + exp(-x))\n",
    "\n",
    "∂σ(x) = σ(x)*(1 - σ(x))\n",
    "\n",
    "\n",
    "#Define a neural network type\n",
    "mutable struct neural_network\n",
    "    #Attributes\n",
    "    WeightMatrix\n",
    "    biasMatrix\n",
    "end\n",
    "\n",
    "# Initialize weights an biases\n",
    "function create_network(inputLayer_Size, hiddenLayer_sizes, outputLayer_size)\n",
    "    WeightMatrix = [[0.0], randn(hiddenLayer_sizes[1], inputLayer_Size)]\n",
    "    biasMatrix = [[0.0], randn(hiddenLayer_sizes[1])]\n",
    "    \n",
    "    for i = 2:length(hiddenLayer_sizes)\n",
    "        push!(WeightMatrix, randn(hiddenLayer_sizes[i], hiddenLayer_sizes[i-1]))\n",
    "        push!(biasMatrix, randn(hiddenLayer_sizes[i]))\n",
    "    end\n",
    "    \n",
    "    push!(WeightMatrix, randn(outputLayer_size, hiddenLayer_sizes[end]))\n",
    "    push!(biasMatrix, randn(outputLayer_size))\n",
    "    \n",
    "    return neural_network(WeightMatrix, biasMatrix)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">With our network function built, we can now create our neural network with four (4) hidden layers of size $150$ and output layer of size $15$. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "---------\n",
      "(150, 784)\n",
      "---------\n",
      "(150, 150)\n",
      "---------\n",
      "(150, 150)\n",
      "---------\n",
      "(150, 150)\n",
      "---------\n",
      "(15, 150)\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "NN = create_network(784, [150, 150, 150, 150], 15);\n",
    "NN.WeightMatrix;\n",
    "NN.biasMatrix;\n",
    "\n",
    "\n",
    "# Prints the size of the hidden layers\n",
    "for w in NN.WeightMatrix\n",
    "    println(size(w))\n",
    "    println(\"---------\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> As we have successfully built our neural network above, we can now begin building the algorithm's infrastructure. To do this, we need several functions to proceed:\n",
    "    \n",
    "   + **Forward Pass function**\n",
    "    \n",
    "        - To pass a given training feature set into the input layer along with a bias neuron\n",
    "    \n",
    "   + **Prediction function**\n",
    "    \n",
    "        - To predict the label of the test image\n",
    "    \n",
    "   + **Success Percentage Calculator Function**; and an\n",
    "    \n",
    "        - To determine the accuracy of our neural network\n",
    "    \n",
    "   + **Error function**\n",
    "        - To calculate the output errors and hidden layer errors\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forwardPass (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forwardPass(network::neural_network, trainingInstance)\n",
    "    Z = [[0.0]] #preactivation values\n",
    "    A = [trainingInstance[1]] #postactivation values\n",
    "    for i = 2:length(network.WeightMatrix)\n",
    "        push!(Z, network.WeightMatrix[i]*A[i-1] + network.biasMatrix[i])\n",
    "        push!(A, σ.(Z[i]))\n",
    "    end\n",
    "    return Z, A\n",
    "end\n",
    "\n",
    "function prediction(network::neural_network, trainingInstance)\n",
    "    Z, A = forwardPass(network, trainingInstance)\n",
    "    return (argmax(A[end]) - 1)\n",
    "end\n",
    "\n",
    "function successPercent(network::neural_network, dataset)\n",
    "    return string(\"Percentage of correctly classified images is: \",\n",
    "        (sum([prediction(network, x) == argmax(x[2]) - 1 ? 1 : 0 for x in dataset])/length(dataset))*100., \" %\")\n",
    "end\n",
    "\n",
    "\n",
    "# Calculate output errors and hidden layer errors\n",
    "function error_deltas(network, trainingInstances)\n",
    "    Z, A = forwardPass(network, trainingInstances)\n",
    "    L = size(network.WeightMatrix)[1]\n",
    "    δ = [(A[end] - trainingInstances[2]).*∂σ.(Z[end])]\n",
    "    \n",
    "    for i = L-1:-1:2\n",
    "        pushfirst!(δ, (network.WeightMatrix[i+1]'*δ[1]).*∂σ.(Z[i]))\n",
    "    end\n",
    "    pushfirst!(δ, [0.0])\n",
    "    return A, δ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Percentage of correctly classified images is: 13.16 %\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the success percentage\n",
    "successPercent(NN, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array{Normed{UInt8,8},1}[[0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8  …  0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8], [0.09N0f8, 0.353N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.871N0f8, 0.0N0f8, 0.059N0f8, 0.0N0f8, 1.0N0f8  …  1.0N0f8, 0.0N0f8, 0.0N0f8, 0.0N0f8, 0.973N0f8, 1.0N0f8, 0.008N0f8, 0.702N0f8, 0.016N0f8, 1.0N0f8], [0.11N0f8, 1.0N0f8, 0.565N0f8, 0.996N0f8, 0.984N0f8, 0.008N0f8, 0.024N0f8, 0.478N0f8, 0.992N0f8, 0.055N0f8  …  0.02N0f8, 0.0N0f8, 0.212N0f8, 0.055N0f8, 0.663N0f8, 0.502N0f8, 0.063N0f8, 0.996N0f8, 0.0N0f8, 0.0N0f8], [1.0N0f8, 0.157N0f8, 0.733N0f8, 0.62N0f8, 0.953N0f8, 1.0N0f8, 0.173N0f8, 0.588N0f8, 0.004N0f8, 0.8N0f8  …  0.953N0f8, 0.043N0f8, 0.0N0f8, 0.0N0f8, 0.027N0f8, 0.02N0f8, 1.0N0f8, 0.051N0f8, 1.0N0f8, 0.239N0f8], [0.949N0f8, 1.0N0f8, 0.0N0f8, 0.204N0f8, 0.502N0f8, 0.047N0f8, 0.639N0f8, 0.71N0f8, 0.537N0f8, 1.0N0f8]], [[0.0], [0.014907379270048957, 0.07048679272644597, 5.710740399966053e-8, -4.105934376839542e-5, 1.5615562206858638e-10, 0.0420319452480772, 5.1180672848439835e-5, 0.005146593296966316, -3.3919203647042404e-5, 6.381547039527129e-6  …  -1.7951014602176866e-6, -5.735758698033071e-5, -6.973808282583566e-7, 1.1115151484636175e-6, 0.0031799557368987577, -1.0740732862959037e-8, 0.0008770186954112364, 0.022544592557173573, -0.0020648477101018045, -8.628578480722379e-6], [-0.023983053223406933, -1.5514892783311824e-7, 0.058855048323377755, -0.0001534024987929746, -0.0005673369138598946, -0.001869937526142039, -0.00014872260020682093, -0.00866208171052605, -0.0004405225115128729, 0.012606145398808014  …  0.0003209200681773108, 4.969727128272306e-5, -0.009295132227040905, 0.00774606262303771, 0.014797449306721808, -0.03182458319729413, -0.01861189713363289, 0.00023663531971744573, 5.195042615016914e-6, -0.0006849464301297135], [8.156051698046919e-5, -0.001828978242892905, 0.059229811117184485, -0.054232426404891676, 0.015780374028319406, -2.15799212261963e-9, 0.042377454403065305, 0.027215694730979595, -0.0004540919147921913, -0.013612288910970254  …  -0.00041869489224254005, 0.014574404179918645, 2.869164577377938e-7, -2.5982188701077977e-5, -0.014494558881843655, 0.0005681326339309015, -0.00011019767315411204, 0.003329039933270511, 0.00011777219614845078, 0.014483790315444181], [0.04471043494108085, 0.00010408058560177041, 0.0, 0.03293652768175756, 0.12549016855332332, -0.04408384212834037, 0.14771576943731032, 0.14655001548607424, 0.1336212532347863, 1.936570980254936e-6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the errors function\n",
    "A_test, δ_test = error_deltas(NN, train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the network\n",
    "\n",
    "<font size=\"3\"> With our functions properly working, we can now delve into training the network for better accuracy. As such we will need to create a random batch of data, or mini batch, and build a function that will update the neural network based on the errors that are generated. The update function will update the weights and biases by utilizing stochastic gradient descent by recursively calculating the gradient of the cost function: \n",
    "\n",
    "\n",
    "$$C(W,b) = \\frac{1}{2} \\sum_{k=1}^n (a_k^L - y_k^{(i)})^2 = \\frac{1}{2} (a_1^3 - y_1^{(i)})^2 + \\frac{1}{2} (a_2^3 - y_2^{(i)})^2 $$\n",
    "\n",
    "<br>\n",
    "To do this, we will focus on the output error:\n",
    "\n",
    "$$\n",
    "\\delta_j^L = \\frac{\\partial C}{\\partial a_j^L}\\sigma'(z_j^L),\n",
    "$$\n",
    "    \n",
    "where the pre-activation value is denoted by $z_j^L$, at neuron $j$, and output error $L$. With this in mind, we will proceed to create our miniature batch and mini batch update functions, as well as our functions that will aid in displaying the results and the test image for testing purposes. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "miniBatch_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_random_miniBatch(miniBatch_size, dataset)\n",
    "    k = rand(1:size(dataset)[1] - miniBatch_size)\n",
    "    return dataset[k:k + miniBatch_size]\n",
    "end\n",
    "\n",
    "function miniBatch_update!(network::neural_network, miniBatch_size::Int64, dataset, α::Float64)\n",
    "    \n",
    "    minibatch = make_random_miniBatch(miniBatch_size, dataset)\n",
    "    L = size(network.WeightMatrix)[1]\n",
    "    \n",
    "    A, δ = error_deltas(network, minibatch[1])\n",
    "    A_batch = []\n",
    "    δ_batch = []\n",
    "    push!(A_batch, A)\n",
    "    push!(δ_batch, δ)\n",
    "    \n",
    "    for i = 2:miniBatch_size # began at 2:\n",
    "        A, δ = error_deltas(network, minibatch[i])\n",
    "        push!(A_batch, A)\n",
    "        push!(δ_batch, δ)\n",
    "    end\n",
    "    \n",
    "    for l = L:-1:2\n",
    "        network.WeightMatrix[l] -= (α/miniBatch_size)*sum([δ_batch[i][l]*A_batch[i][l-1]' for i = 1:miniBatch_size])\n",
    "        network.biasMatrix[l] -= (α/miniBatch_size)*sum([δ_batch[i][l] for i = 1:miniBatch_size])\n",
    "    end\n",
    "end\n",
    "\n",
    "function showTestImg(i)\n",
    "    colorview(Gray, test_x[:,:,i]')\n",
    "end\n",
    "\n",
    "function showTest_example(network, i, test_data)\n",
    "    println(\"Predict Label: \", prediction(network, test_data[i]))\n",
    "    println(\"Actual Label: \", argmax(test_data[i][2])-1)\n",
    "    showTestImg(i)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Test the data\n",
    "\n",
    "<font size=\"3\"> To ensure that all of our functions are working properly, we will implement and test our deep neural network on the MNIST test data as seen in the following code cells.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly classified images is: 13.16 %\n",
      "\n",
      "\n",
      "Predict Label: 0\n",
      "Actual Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAGmSURBVGje7dmxS5VRHMbxjyEIkoPOgpqFgoP1FwRNLtnUGBJEQWuTm4OYkUMQLS2NIQ0NhS1BCBFE0RJNQUO0pIlNDYXY8F65S8O5J/Ce98d5lpf7wrlfvjycwznnpaampqampqam/RnIHTiFK1jGb0zgZ8K4E8dtGB84mDNoHJs4hxWM4RQ+lGgYH9hzh5c1/X3BJWyVbhgf2NNaOoPnuIVfeNkGw/jA5Hk4gle4g2c4bIthfGDyPFzDPBZx0Hk3iEl81exrijSMD0yah0P4g1Xd/mZwFvdxA09LNYwPTOpwAu/wtvN7AefxGI/wo2TD+MCktXQOtzXnv0PcxXfs4iHed55FGsYHJs3DT7im6W4DO5r+jrKb8if9MowPTN6X7mDpH+8H1A77DMy+a4OruKdZa7+VahgfmHXXdpQL2Mao2mEfgdkdnsEw1vGxZMP4wKwOR/AC+7rnxWIN4wOzOpzFNF7rnhmLNYwPzOrwuuZ+7UEbDOMDs/alb7CHi20wjA/seR7exGl8bothfGDP8/AJTmq+/6Z+q+irYXxgTU1NTc3/5y9LxTuYwSFWTQAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 reinterpret(Gray{N0f8}, ::LinearAlgebra.Adjoint{Normed{UInt8,8},Array{Normed{UInt8,8},2}}):\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(successPercent(NN, test_data), \"\\n\\n\")\n",
    "i = rand([x for x = 1:10000])\n",
    "showTest_example(NN, i, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> As our success percentage is still low, we will update our function and run it again to ensure better accuracy. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly classified images is: 75.82 %\n",
      "\n",
      "\n",
      "Predict Label: 9\n",
      "Actual Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAG5SURBVGje7di9a1RBGMXhZ0NQSGnsTKlNwJA0goL/gZDGwsLWThBJCCIExEJBsNBGRLGyENFCxMpGUBBETFLESgh2FhJE0cIvLGYD44W9bHbFzL7Mae6888GPw2Hu3LlUVVVVVVVVVVVV7bw6vQb24wxO4yGu4uU/AI79b4fxgT0zPIZHWf0NT/E46/uK+5jFOn6U6DA+sGeGh/AEe1oW/8Q77JX26wZel+YwPrDTNvgMR7vt27iLOZxozNuHKXzEPekdXIzD+MDxtsHNrP0FL/Ac1xvzlnFB2o8zpTmMD2zN8Armu+2zOI/vo+awAiuwfOD4diZfwuKoOYwPbM3wjXRveCDdF6cxId0ztrQbC1m9UprD+MBOP5MWpLORdNdfysYOYjWrD+NVSQ7jA/t6l17DWrf9IevfhXNZ/cnfe7QIh/GBfe3DXprC+6y+g1OlOYwP3NY3TVPzjfpyiQ7jA4fK8MgoOIwPHDjDGRzP6rf4XKLD+MCBM5xtLL4h/S8tzmF84MAZnuw+f+MibpbqMD5wqPNwSxv4VarD+MChMxzDgZIdxgcOnOEtTErnYtEO4wOrqqp2Xn8APbE2/GQgUywAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 reinterpret(Gray{N0f8}, ::LinearAlgebra.Adjoint{Normed{UInt8,8},Array{Normed{UInt8,8},2}}):\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ = 1:10000\n",
    "    miniBatch_update!(NN, 2, train_data, 0.4)\n",
    "end\n",
    "\n",
    "println(successPercent(NN, test_data), \"\\n\\n\")\n",
    "i = rand([x for x = 1:10000])\n",
    "showTest_example(NN, i, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> Now we can see that the update was successful since the success percentage made a significant increase. We will continue to update our function and run more tests to attempt to get our network within 90% of successfully classified images. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly classified images is: 89.55 %\n",
      "\n",
      "\n",
      "Predict Label: 6\n",
      "Actual Label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAHtSURBVGje7djPi05RGAfwD6kZ5Vdhyk7yczFKmZ0kqXepJj8Wyiw0M/8BsrCQYqEUyQbJjrJQZGNjS0l+lLK2oPwWdmNxbnm95L33nWne8z6d7+be23k6377n2/M857kUDDoWzMUmLZzFthqxC+dbYXzCRbPdYCfO4WuuCuMTzsrDFm7hJXblqjA+Yc+1dAUeYgv24n6uCuMT9pyHxzCK6+r71xeF8Ql7ysM1eCL1wDF8zllhfMLGeTiE8xjBKc3864vC+ISN8nAIN7APVzE1CArjEzbycBXe4h22480gKIxP2KiWbqyeJ/XmX18UxiesnYcb8AjLsRrv29Za2I/X0h31WU4K4xPWzsMRLMM9fJLmwymMSzPikipuWqqzH3JRGJ+wtod78AWTUs5dxErMdMStlfK0eDhvqOXhmDTTv8JWXMHiau0ObuJxFTOZm8L4hLU8HMUwLvjzn8wZnMaP6nta8TBXD3dIl59r1fd3TOD2P2K7XZLiH2l+Hq7DAanvzeAFDkp1tROH/N0f+64wPmFXD9f73ft+4hI2SbX1aVvcUpyQZo5vOSmMT9jVw91t78O4LP2n+YijuFutHcZmqV/+b/6Pf6T5ediJB9LsMIHn0j11HMer9SO5KYxPWFBQUFBQUFBQUFBQMBf4BcbgQ/7y2Jb3AAAAAElFTkSuQmCC",
      "text/plain": [
       "28×28 reinterpret(Gray{N0f8}, ::LinearAlgebra.Adjoint{Normed{UInt8,8},Array{Normed{UInt8,8},2}}):\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ = 1:10000\n",
    "    miniBatch_update!(NN, 2, train_data, 0.4)\n",
    "end\n",
    "\n",
    "println(successPercent(NN, test_data), \"\\n\\n\")\n",
    "i = rand([x for x = 1:10000])\n",
    "showTest_example(NN, i, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly classified images is: 91.28 %\n",
      "\n",
      "\n",
      "Predict Label: 5\n",
      "Actual Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAHLSURBVGje7dnPiwhhHMfx1y5KHFwkcpDLlouLHOTgQihte6C25G9QykVtkvIjNynFjRwcHLjsYdVehKRNbXY5kAtlleRHKVkOz4gDu/NQ5plvz+f0zEzPfObdp+88852Hqqqqqqqqqqqqqu418K83WIet2Nkcj2J1c+NreIEreIc5DP5vwviGf5XhKizBdlyUclxMF3C4C8L4hktzJwzgAE5gTfPE8/iKV7jxh3l3uyKMb5hdh/twsxm/wTROYbJUwviG2Rnel9a/l9iDmdIJ4xtmvUvHsAWvsVd+fp0QxjdsXYfr8aCZMImDfSGMb9iqDscwgrU4i2N9Ioxv2KoOn0rfMh/xHp/6RBjfsFUdLsMT3MY47jXnp/CldML4hq3q8AyO/ub8JZzD85IJ4xu2ynAFjkhr4hBW/nLtDnaUTBjfMLs/3IRhqa8nfatuK5kwvmH2v7ZZLPczw+ulE8Y3zM5wELua8WdMlE4Y3zA7w/043YzP43HphPENF10PN+Ct1F+cxG5sxDPpv9uH0gnjGy6Y4UNsxlUckvYMv0k94oi031Q8YXzDBTOclzL7oTmpJzzeJ8L4hq3Ww0e4hcvSPm+vCOMbVlVVda/vb4VAtZHOtGIAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 reinterpret(Gray{N0f8}, ::LinearAlgebra.Adjoint{Normed{UInt8,8},Array{Normed{UInt8,8},2}}):\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.086)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.569)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.318)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                     \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)    Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ = 1:10000\n",
    "    miniBatch_update!(NN, 2, train_data, 0.4)\n",
    "end\n",
    "\n",
    "println(successPercent(NN, test_data), \"\\n\\n\")\n",
    "i = rand([x for x = 1:10000])\n",
    "showTest_example(NN, i, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "<font size=\"3\">With our neural network within the 90% accuracy, we can see that we have built a successful neural network that predicts the label of our test images. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### For additional information, please see the following:\n",
    "<br>\n",
    "<font size=\"3\">\n",
    "\n",
    "+ [Artifical Neural Network - Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "<br>\n",
    "    \n",
    "+ [Neural Network & Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "<br>\n",
    "\n",
    "+ [Understanding Neural Networks](https://towardsdatascience.com/understanding-neural-networks-19020b758230)\n",
    "    \n",
    "</font>\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
